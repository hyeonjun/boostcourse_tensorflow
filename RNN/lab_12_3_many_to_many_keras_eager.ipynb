{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab-12-3-many-to-many-keras-eager.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPSoXL7fg/GpoSd9+yarGqc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyeonjun/boostcourse_tensorflow/blob/master/lab_12_3_many_to_many_keras_eager.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmbXpM748xhD"
      },
      "source": [
        "# many to many\n",
        "\n",
        "simple pos-targger training\n",
        "- many to many\n",
        "- variable input sequence length\n",
        "\n",
        "Example : part of speech tagging\n",
        "- Preparing dataset\n",
        "- Creating and training model\n",
        "- Checking performance\n",
        "\n",
        "자연어 처리에서 개체명 인식 또는 형태소 분석과 같은 시퀀스 태깅을 모델링하는데 활용할 수 있다.\n",
        "\n",
        "many to many는 RNN이 시퀀스를 구성하고 있는 각각의 토큰에 대해 모두 출력을 내어주는 구조.\n",
        "\n",
        "setence를 word 단위로 tokenization 한 뒤,\n",
        "이러한 토큰으로 이루어진 시퀀스를 RNN이 각각의 토큰을 읽고 해당 토큰이 어떤 품사인지 파악하는 방식으로 many to many를 활용할 수 있다.\n",
        "\n",
        "토큰인 word는 숫자가 아니기 때문에 Embedding layer를 이용하여 RNN이 처리할 수 있도록 numeric vector로 변환해야 한다.\n",
        "\n",
        "이렇게 numeric vector로 변환된 토큰을 RNN이 각각의 토큰ㅇ르 순서대로 읽을 때마다 토큰에 대한 출력을 내고 이를 정답과 비교하여 토큰마다 loss를 계산한다.\n",
        "\n",
        "그리고 모든 토큰에 대해 계산된 loss의 평균을 낸다(sequence loss)\n",
        "\n",
        "이 sequence loss를 RNN을 back propagation을 통해서 학습할 수 있다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eBrAMI88pDC"
      },
      "source": [
        "# setup\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from pprint import pprint\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGIdefcU9BFD"
      },
      "source": [
        "# Preparing dataset\n",
        "# example data\n",
        "# 문장\n",
        "sentences = [['I', 'feel', 'hungry'],\n",
        "     ['tensorflow', 'is', 'very', 'difficult'],\n",
        "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
        "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
        "# 품사\n",
        "pos = [['pronoun', 'verb', 'adjective'],\n",
        "     ['noun', 'verb', 'adverb', 'adjective'],\n",
        "     ['noun', 'verb', 'determiner', 'noun', 'preposition', 'adjective', 'noun'],\n",
        "     ['noun', 'verb', 'adverb', 'adjective', 'verb']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxvoJjSQ9Dk1",
        "outputId": "f0bca87a-6d52-4978-f79b-ab59593a6adb"
      },
      "source": [
        "# Preprocessing dataset\n",
        "# creating a token dictionary for word\n",
        "word_list = sum(sentences, [])\n",
        "word_list = sorted(set(word_list))\n",
        "word_list = ['<pad>']+word_list # 입력과 출력의 같음을 표현하기 위해 pad 토큰 포함\n",
        "word2idx = {word : idx for idx, word in enumerate(word_list)}\n",
        "idx2word = {idx : word for idx, word in enumerate(word_list)}\n",
        "\n",
        "print(word2idx)\n",
        "print(idx2word)\n",
        "print(len(idx2word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<pad>': 0, 'I': 1, 'a': 2, 'changing': 3, 'deep': 4, 'difficult': 5, 'fast': 6, 'feel': 7, 'for': 8, 'framework': 9, 'hungry': 10, 'is': 11, 'learning': 12, 'tensorflow': 13, 'very': 14}\n",
            "{0: '<pad>', 1: 'I', 2: 'a', 3: 'changing', 4: 'deep', 5: 'difficult', 6: 'fast', 7: 'feel', 8: 'for', 9: 'framework', 10: 'hungry', 11: 'is', 12: 'learning', 13: 'tensorflow', 14: 'very'}\n",
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qDqTMoe9jnr",
        "outputId": "b0defd64-b637-4806-a8cc-6a9e32e368cd"
      },
      "source": [
        "# creating a token dictionary for part of speech\n",
        "pos_list = sum(pos, [])\n",
        "pos_list = sorted(set(pos_list))\n",
        "pos_list = ['<pad>']+pos_list # 입력과 출력의 같음을 표현하기 위해 pad 토큰 포함\n",
        "pos2idx = {pos : idx for idx, pos in enumerate(pos_list)}\n",
        "idx2pos = {idx : pos for idx, pos in enumerate(pos_list)}\n",
        "\n",
        "print(pos2idx)\n",
        "print(idx2pos)\n",
        "print(len(pos2idx))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<pad>': 0, 'adjective': 1, 'adverb': 2, 'determiner': 3, 'noun': 4, 'preposition': 5, 'pronoun': 6, 'verb': 7}\n",
            "{0: '<pad>', 1: 'adjective', 2: 'adverb', 3: 'determiner', 4: 'noun', 5: 'preposition', 6: 'pronoun', 7: 'verb'}\n",
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxWbOk4j-FTD",
        "outputId": "0c0956ba-7bab-4bba-e4cf-a87f2ed586aa"
      },
      "source": [
        "# converting sequence of tokens to sequence of indices\n",
        "max_sequence = 10\n",
        "# 토큰 딕셔너리를 기반으로 word의 시퀀스를 integer index의 시퀀스로 변환\n",
        "x_data = list(map(lambda sentence : [word2idx.get(token) for token in sentence], sentences))\n",
        "# 품사도 마찬가지로 변환\n",
        "y_data = list(map(lambda sentence : [pos2idx.get(token) for token in sentence], pos))\n",
        "\n",
        "# padding the sequence of indices\n",
        "# pad_sequences function을 이용하여 max_sequence의 값만큼의 길이로 padding\n",
        "x_data = pad_sequences(sequences=x_data, maxlen=max_sequence, padding='post')\n",
        "# padding한 부분에 대한 마스킨 정보를 담고 있는 x_data_mask\n",
        "x_data_mask = ((x_data != 0) * 1).astype(np.float32)\n",
        "# 각각의 sentence가 몇 개의 word로 tokenization이 됐는지 계산한 sentence에 유효한  길이\n",
        "x_data_len = list(map(lambda sentence : len(sentence), sentences))\n",
        "\n",
        "\n",
        "y_data = pad_sequences(sequences=y_data, maxlen=max_sequence, padding='post')\n",
        "\n",
        "# checking data\n",
        "print(x_data, x_data_len)\n",
        "print(x_data_mask)\n",
        "print(y_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1  7 10  0  0  0  0  0  0  0]\n",
            " [13 11 14  5  0  0  0  0  0  0]\n",
            " [13 11  2  9  8  4 12  0  0  0]\n",
            " [13 11 14  6  3  0  0  0  0  0]] [3, 4, 7, 5]\n",
            "[[1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0.]]\n",
            "[[6 7 1 0 0 0 0 0 0 0]\n",
            " [4 7 2 1 0 0 0 0 0 0]\n",
            " [4 7 3 4 5 1 4 0 0 0]\n",
            " [4 7 2 1 7 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxH9MNa__aQK",
        "outputId": "968232b8-5944-4edc-f3e4-3a556be57371"
      },
      "source": [
        "# Creating model\n",
        "# creating rnn for \"many to many\" sequence tagging\n",
        "num_classes = len(pos2idx)\n",
        "hidden_dim=10\n",
        "input_dim=len(word2idx)\n",
        "output_dim=len(word2idx)\n",
        "one_hot = np.eye(len(word2idx))\n",
        "\n",
        "model = Sequential()\n",
        "# Embedding layer의 경우 토큰을 one hot venctor로 표현하고, 학습은 시키지 않는다.\n",
        "# 0으로 padding된 부분을 연산에서 제외하는 방식으로 활용\n",
        "model.add(layers.Embedding(input_dim=input_dim, output_dim=output_dim, mask_zero=True,\n",
        "                           trainable=False, input_length=max_sequence, embeddings_initializer=keras.initializers.Constant(one_hot)))\n",
        "# Embedding layer 이외에는 Simple RNN을 return_sequences=True 옵션을 줘서 활용\n",
        "# RNN이 있는 모든 토큰에 대해서 출력을 내줘야하기 때문.\n",
        "model.add(layers.SimpleRNN(units=hidden_dim, return_sequences=True))\n",
        "# TImeDistributed와 Dense를 이용.\n",
        "# 매 토큰마다 품사가 무엇인지 classification을 하는 형태로 RNN을 many to many 방식으로\n",
        "# 활용하는 구조를 완성할 수 있다.\n",
        "model.add(layers.TimeDistributed(layers.Dense(units=num_classes)))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 10, 15)            225       \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       (None, 10, 10)            260       \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 10, 8)             88        \n",
            "=================================================================\n",
            "Total params: 573\n",
            "Trainable params: 348\n",
            "Non-trainable params: 225\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPhKNfMWAFUa"
      },
      "source": [
        "# Training model\n",
        "# creating loss Function 모델의 출력과 결과를 비교.\n",
        "# 매 토큰마다 loss를 계산해야하고 특히 pad 토큰에 대한 부분은 loss에 반영하면 안된다.\n",
        "# 그래서 실제 시퀀스에 유효한 길이와 max_sequence를 받아 마스킹을 생성하고\n",
        "# 이를 mini batch loss에 반영하는 형태로 loss function을 구현한다.\n",
        "def loss_fn(model, x, y, x_len, max_sequence):\n",
        "  masking = tf.sequence_mask(x_len, maxlen=max_sequence, dtype=tf.float32)\n",
        "  valid_time_step=tf.cast(x_len, dtype=tf.float32)\n",
        "  sequence_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "      y_true=y, y_pred=model(x), from_logits=True) * masking\n",
        "  sequence_loss = tf.reduce_sum(sequence_loss, axis=-1) / valid_time_step\n",
        "  sequence_loss = tf.reduce_mean(sequence_loss)\n",
        "  return sequence_loss\n",
        "\n",
        "# creating and optimizer\n",
        "lr = 0.1\n",
        "epochs = 30\n",
        "batch_size = 2 \n",
        "opt = tf.keras.optimizers.Adam(learning_rate = lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70Ibgsz3Azts",
        "outputId": "52e03980-6f70-486e-be00-c82450c5fc9a"
      },
      "source": [
        "# generating data pipeline\n",
        "tr_dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data, x_data_len))\n",
        "tr_dataset = tr_dataset.shuffle(buffer_size=4)\n",
        "tr_dataset = tr_dataset.batch(batch_size = 2)\n",
        "\n",
        "print(tr_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((None, 10), (None, 10), (None,)), types: (tf.int32, tf.int32, tf.int32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd-RwQsZCSya",
        "outputId": "26a3c0c8-b5f6-42c4-db79-dd820d42157b"
      },
      "source": [
        "# training\n",
        "tr_loss_hist = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  avg_tr_loss = 0\n",
        "  tr_step = 0\n",
        "\n",
        "  for x_mb, y_mb, x_mb_len in tr_dataset:\n",
        "    with tf.GradientTape() as tape: # mini batch마다의 시퀀스 loss를 계산\n",
        "      # 시퀀스의 유효한 길이(x_len)와 max_sequence를 인풋으로 받고있음\n",
        "      tr_loss = loss_fn(model, x=x_mb, y=y_mb, x_len=x_mb_len, max_sequence=max_sequence)\n",
        "    grads = tape.gradient(target=tr_loss, sources=model.variables) # Gradient 계산\n",
        "    opt.apply_gradients(grads_and_vars=zip(grads, model.variables)) # Gradient Descent\n",
        "    avg_tr_loss += tr_loss\n",
        "    tr_step += 1\n",
        "  if (epoch + 1) %5 == 0:\n",
        "    print('epoch : {:3}, tr_loss : {:.3f}'.format(epoch +1, avg_tr_loss))\n",
        "  else:\n",
        "    avg_tr_loss /= tr_step\n",
        "    tr_loss_hist.append(avg_tr_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch :   5, tr_loss : 0.397\n",
            "epoch :  10, tr_loss : 0.017\n",
            "epoch :  15, tr_loss : 0.004\n",
            "epoch :  20, tr_loss : 0.002\n",
            "epoch :  25, tr_loss : 0.001\n",
            "epoch :  30, tr_loss : 0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKcLrvECDThV",
        "outputId": "a6eec3e3-1be0-45a9-c31e-df71e3338242"
      },
      "source": [
        "# Checking performance\n",
        "yhat = model.predict(x_data)\n",
        "yhat = np.argmax(yhat, axis=-1) * x_data_mask\n",
        "\n",
        "# pad 토큰이 추가되었는데 이는 모델을 batch 단위 연산으로 트레이닝하기 위해서\n",
        "# 입력과 출력의 시퀀스에 pad 토큰을 이용하여 padding했기 때문.\n",
        "pprint(list(map(lambda row : [idx2pos.get(elm) for elm in row],yhat.astype(np.int32).tolist())), width=120)\n",
        "pprint(pos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['pronoun', 'verb', 'adjective', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'],\n",
            " ['noun', 'verb', 'adverb', 'adjective', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'],\n",
            " ['noun', 'verb', 'determiner', 'noun', 'preposition', 'adjective', 'noun', '<pad>', '<pad>', '<pad>'],\n",
            " ['noun', 'verb', 'adverb', 'adjective', 'verb', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n",
            "[['pronoun', 'verb', 'adjective'],\n",
            " ['noun', 'verb', 'adverb', 'adjective'],\n",
            " ['noun', 'verb', 'determiner', 'noun', 'preposition', 'adjective', 'noun'],\n",
            " ['noun', 'verb', 'adverb', 'adjective', 'verb']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia2_V8qtDplr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "331552c2-ee52-4185-ef8a-4a74a378b197"
      },
      "source": [
        "plt.plot(tr_loss_hist)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f06b1f5ff50>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}